{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 【飞桨学习赛：MarTech Challenge 点击反欺诈预测】第三名方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一.方案介绍：\n",
    "\n",
    "## 1.1 赛题介绍\n",
    "广告欺诈是数字营销需要面临的重要挑战之一，点击会欺诈浪费广告主大量金钱，同时对点击数据会产生误导作用。点击欺诈预测适用于各种信息流广告投放，banner广告投放，以及百度网盟平台，帮助商家鉴别点击欺诈，锁定精准真实用户。\n",
    "\n",
    "本次比赛提供了约50万次点击数据，数据中对某些特征含义进行了隐藏并进行了脱敏处理。测试集中提供了会话sid及该会话的各维度特征值，选手需要基于训练集得出的模型进行预测，判断该会话sid是否为正常点击，还是作弊行为。\n",
    "\n",
    "\n",
    "## 1.2 项目说明：\n",
    "\n",
    "点击反欺诈预测是一个二分类问题，判断会话是否为正常点击，还是作弊行为。本次数据信息上数字特征列较多，而树模型对结构化的数据可以更好地拟合，可以很好适用于数据量不大且类别信息较多时。本项目主要基于Catboost树模型结合特征工程构建点击反欺诈预测模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二.数据查看与分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 解压比赛数据集\r\n",
    "# !unzip /home/aistudio/data/data105017/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 使用pandas读取数据\r\n",
    "import pandas as pd\r\n",
    "train = pd.read_csv('/home/aistudio/train.csv')  # 训练数据\r\n",
    "test = pd.read_csv('/home/aistudio/test.csv')    # 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看训练数据，共500000条、21列\r\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看训练数据文件信息及每列的类型，由于数据量较小故并无考虑做较多内存优化\r\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "| 字段 | 类型| 说明|\n",
    "|---|---|---|\n",
    "|sid|string|样本id/请求会话sid| \n",
    "|package|string|媒体信息，包名（已加密）| \n",
    "|version|string|媒体信息，app版本| \n",
    "|android_id|string|媒体信息，对外广告位ID（已加密）| \n",
    "|media_id|string|媒体信息，对外媒体ID（已加密）| \n",
    "|apptype|int|媒体信息，app所属分类| \n",
    "|timestamp|bigint|请求到达服务时间，单位ms| \n",
    "|location|int|用户地理位置编码（精确到城市）| \n",
    "|fea_hash|int|用户特征编码（具体物理含义略去）| \n",
    "|fea1_hash|int|用户特征编码（具体物理含义略去）| \n",
    "|cus_type|int|用户特征编码（具体物理含义略去）| \n",
    "|ntt|int|网络类型 0-未知, 1-有线网, 2-WIFI, 3-蜂窝网络未知, 4-2G, 5-3G, 6–4G| \n",
    "|carrier|string|设备使用的运营商 0-未知, 46000-移动, 46001-联通, 46003-电信| \n",
    "|os|string|操作系统，默认为android| \n",
    "|osv|string|操作系统版本| \n",
    "|lan|string|设备采用的语言，默认为中文| \n",
    "|dev_height|int|设备高| \n",
    "|dev_width|int|设备宽| \n",
    "|dev_ppi|int|屏幕分辨率| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 检测每列缺失值情况\r\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 对特征列进行相关性分析\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "import seaborn as sns\r\n",
    "plt.figure(figsize=(10,10))\r\n",
    "sns.heatmap(train.corr(),cbar=True,annot=True,cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.1 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 下载catboost库\r\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导入所需的第三方库\r\n",
    "import sys\r\n",
    "import numpy as np\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.multioutput import MultiOutputRegressor\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn import preprocessing\r\n",
    "from sklearn import preprocessing\r\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\r\n",
    "from catboost import CatBoostClassifier\r\n",
    "import gc\r\n",
    "import re\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2 特征工程\n",
    "\n",
    "1.特征交互：特征和特征之间组合、特征和特征之间衍生\n",
    "\n",
    "2.特征编码：one-hot编码、label-encode编码等\n",
    "\n",
    "3.特征选择：通过对特征重要性及相关性的分析，筛选掉无用的特征\n",
    "\n",
    "特征工程很大程度上是在帮助模型学习，数据和特征决定模型上限。在模型学习不好的地方或者难以学习的地方，通过特征工程的方式帮助其学习，通过人为筛选、人为构建组合特征让模型原本很难学好的东西可以更加容易地进行学习、进而拿到更好的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义所需的特征工程处理函数：\r\n",
    "\r\n",
    "# 数据特征处理和类别转换\r\n",
    "def data_preprocessing(train, test1):\r\n",
    "    df1 = train.drop(['Unnamed: 0'], axis=1)\r\n",
    "    df2 = test1.drop(['Unnamed: 0'], axis=1)\r\n",
    "    df2[\"label\"] = -1\r\n",
    "    li = []\r\n",
    "    for df_index in [df1,df2]:\r\n",
    "        # 类型转换\r\n",
    "        for col in [\"android_id\", \"apptype\", \"carrier\", \"ntt\", \"media_id\", \"cus_type\", \"package\", 'fea1_hash', \"location\"]:\r\n",
    "            df_index[col] = df_index[col].astype(\"object\")\r\n",
    "        for col in [\"fea_hash\"]:\r\n",
    "            df_index[col] = df_index[col].map(lambda x: 0 if len(str(x)) > 16 else int(x))\r\n",
    "        for col in [\"dev_height\", \"dev_ppi\", \"dev_width\", \"fea_hash\", \"label\"]:\r\n",
    "            df_index[col] = df_index[col].astype(\"int64\")\r\n",
    "        # 时间特征处理和转换\r\n",
    "        df_index[\"truetime\"] = pd.to_datetime(df_index['timestamp'], unit='ms', origin=pd.Timestamp('1970-01-01'))\r\n",
    "        df_index[\"day\"] = df_index.truetime.dt.day\r\n",
    "        df_index[\"hour\"] = df_index.truetime.dt.hour\r\n",
    "        df_index[\"minute\"] = df_index.truetime.dt.minute\r\n",
    "        df_index.set_index(\"sid\", drop=True, inplace=True)\r\n",
    "        df_index.dev_height[df_index.dev_height == 0] = None\r\n",
    "        df_index.dev_width[df_index.dev_width == 0] = None\r\n",
    "        df_index.dev_ppi[df_index.dev_ppi == 0] = None\r\n",
    "        li.append(df_index)\r\n",
    "    df2[\"label\"] = None\r\n",
    "    return li\r\n",
    "\r\n",
    "# 类别预处理\r\n",
    "def process_category(df1, df2, col):\r\n",
    "    le = preprocessing.LabelEncoder()  # 特征编码\r\n",
    "    df1[col] = le.fit_transform(df1[col])\r\n",
    "    df1[col] = df1[col].astype(\"object\")\r\n",
    "    df2[col] = le.transform(df2[col])\r\n",
    "    df2[col] = df2[col].astype(\"object\")\r\n",
    "    return df1, df2\r\n",
    "\r\n",
    "\r\n",
    "def dict_category(df1, df2, col, dict1):\r\n",
    "    print(col, dict1)\r\n",
    "    df1[col] = df1[col].map(dict1)\r\n",
    "    df1[col] = df1[col].astype(\"object\")\r\n",
    "    df2[col] = df2[col].map(dict1)\r\n",
    "    df2[col] = df2[col].astype(\"object\")\r\n",
    "    return df1, df2\r\n",
    "\r\n",
    "def filter_value(df1, df2, col, top, other=-1):\r\n",
    "    set1 = set(df1[col].value_counts().head(top).index)\r\n",
    "    def process_temp(x):\r\n",
    "        if x in set1:\r\n",
    "            return x\r\n",
    "        else:\r\n",
    "            return other\r\n",
    "    df1[col] = df1[col].apply(process_temp)\r\n",
    "    df2[col] = df2[col].apply(process_temp)\r\n",
    "    return df1, df2\r\n",
    "\r\n",
    "def special_category(df1, df2, col):\r\n",
    "    if col == \"apptype\":\r\n",
    "        df1, df2 = filter_value(df1, df2, col, 75, -1)\r\n",
    "    if col == \"media_id\":\r\n",
    "        df1, df2 = filter_value(df1, df2, col, 200, -1)\r\n",
    "    if col == \"version\":\r\n",
    "        df2[col] = df2[col].replace(\"20\", \"0\").replace(\"21\", \"0\")\r\n",
    "    if col == \"lan\":\r\n",
    "        def foreign_lan(x):\r\n",
    "            set23 = {'zh-CN', 'zh', 'cn', 'zh_CN', 'Zh-CN', 'zh-cn', 'ZH', 'CN', 'zh_CN_#Hans'}\r\n",
    "            if x in set23:\r\n",
    "                return 0\r\n",
    "            elif x == \"unk\":\r\n",
    "                return 2\r\n",
    "            else:\r\n",
    "                return 1\r\n",
    "        df1[\"vpn\"] = df1[\"lan\"].apply(foreign_lan)\r\n",
    "        df2[\"vpn\"] = df2[\"lan\"].apply(foreign_lan)\r\n",
    "        set12 = {'zh-CN', 'zh', 'cn', 'zh_CN', 'Zh-CN', 'zh-cn', 'ZH', 'CN', 'tw', 'en', 'zh_CN_#Hans', 'ko'}\r\n",
    "        def process_lan(x):\r\n",
    "            if x in set12:\r\n",
    "                return x\r\n",
    "            else:\r\n",
    "                return \"unk\"\r\n",
    "        df1[col] = df1[col].apply(process_lan)\r\n",
    "        df2[col] = df2[col].apply(process_lan)\r\n",
    "    if col == \"package\":\r\n",
    "        df1, df2 = filter_value(df1, df2, col, 800, -1)\r\n",
    "    if col == \"fea1_hash\":\r\n",
    "        df1, df2 = filter_value(df1, df2, col, 850, -1)\r\n",
    "    if col == \"fea_hash\":\r\n",
    "        df1, df2 = filter_value(df1, df2, col, 850, -1)\r\n",
    "    df1, df2 = process_category(df1, df2, col)\r\n",
    "    return df1, df2\r\n",
    "\r\n",
    "\r\n",
    "def feature(df1, df2):\r\n",
    "    def divided(x):\r\n",
    "        if x % 40 == 0:\r\n",
    "            return 2\r\n",
    "        elif not x:\r\n",
    "            return 1\r\n",
    "        else:\r\n",
    "            return 0\r\n",
    "\r\n",
    "    # 特征构造\r\n",
    "    df1[\"160_height\"] = df1.dev_height.apply(divided)\r\n",
    "    df2[\"160_height\"] = df2.dev_height.apply(divided)\r\n",
    "    df1[\"160_width\"] = df1.dev_width.apply(divided)\r\n",
    "    df2[\"160_width\"] = df2.dev_width.apply(divided)\r\n",
    "    df1[\"160_ppi\"] = df1.final_ppi.apply(divided)\r\n",
    "    df2[\"160_ppi\"] = df2.final_ppi.apply(divided)\r\n",
    "    df1[\"hw_ratio\"] = df1.dev_height / df1.dev_width\r\n",
    "    df2[\"hw_ratio\"] = df2.dev_height / df2.dev_width\r\n",
    "    df1[\"hw_matrix\"] = df1.dev_height * df1.dev_width\r\n",
    "    df2[\"hw_matrix\"] = df2.dev_height * df2.dev_width\r\n",
    "    df1[\"inch\"] = (df1.dev_height ** 2 + df1.dev_width ** 2) ** 0.5 / df1.final_ppi\r\n",
    "    df2[\"inch\"] = (df2.dev_height ** 2 + df2.dev_width ** 2) ** 0.5 / df2.final_ppi\r\n",
    "    return df1, df2\r\n",
    "\r\n",
    "\r\n",
    "def rf_cast(df1, df2):\r\n",
    "    c1 = df1.dev_width.notnull()\r\n",
    "    c2 = df1.dev_height.notnull()\r\n",
    "    c3 = df1.dev_ppi.isna()\r\n",
    "    c4 = df1.dev_ppi.notnull()\r\n",
    "    df1[\"mynull1\"] = c1 & c2 & c3\r\n",
    "    df1[\"mynull2\"] = c1 & c2 & c4\r\n",
    "\r\n",
    "    predict = df1[\r\n",
    "        [\"apptype\", \"carrier\", \"dev_height\", \"dev_ppi\", \"dev_width\", \"media_id\", \"ntt\", \"mynull1\", \"mynull2\"]]\r\n",
    "\r\n",
    "    df_notnans = predict[predict.mynull2 == True]\r\n",
    "\r\n",
    "    # 75训练25预测\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\r\n",
    "        df_notnans[[\"apptype\", \"carrier\", \"dev_height\", \"dev_width\", \"media_id\", \"ntt\"]], df_notnans[\"dev_ppi\"],\r\n",
    "        train_size=0.75, random_state=6)\r\n",
    "    \r\n",
    "    # 随机森林分类\r\n",
    "    regr_multirf = RandomForestClassifier(n_estimators=100, max_depth=40, random_state=0, n_jobs=-1)\r\n",
    "    regr_multirf.fit(X_train, y_train)\r\n",
    "    score = regr_multirf.score(X_test, y_test)\r\n",
    "    print(\"prediction score is {:.2f}%\".format(score * 100))\r\n",
    "    df_nans = predict[predict.mynull1 == True].copy()\r\n",
    "    df_nans[\"dev_ppi_pred\"] = regr_multirf.predict(\r\n",
    "        df_nans[[\"apptype\", \"carrier\", \"dev_height\", \"dev_width\", \"media_id\", \"ntt\"]])\r\n",
    "    df1 = pd.merge(df1, df_nans[[\"dev_ppi_pred\"]], on=\"sid\", how=\"left\")\r\n",
    "    c1 = df2.dev_width.notnull()\r\n",
    "    c2 = df2.dev_height.notnull()\r\n",
    "    c3 = df2.dev_ppi.isna()\r\n",
    "    c4 = df2.dev_ppi.notnull()\r\n",
    "    df2[\"mynull1\"] = c1 & c2 & c3\r\n",
    "    df2[\"mynull2\"] = c1 & c2 & c4\r\n",
    "    predict_test = df2[\r\n",
    "        [\"apptype\", \"carrier\", \"dev_height\", \"dev_ppi\", \"dev_width\", \"media_id\", \"ntt\", \"mynull1\", \"mynull2\"]]\r\n",
    "    df_nans = predict_test[predict_test.mynull1 == True].copy()\r\n",
    "    df_nans[\"dev_ppi_pred\"] = regr_multirf.predict(\r\n",
    "        df_nans[[\"apptype\", \"carrier\", \"dev_height\", \"dev_width\", \"media_id\", \"ntt\"]])\r\n",
    "    df2 = pd.merge(df2, df_nans[[\"dev_ppi_pred\"]], on=\"sid\", how=\"left\")\r\n",
    "\r\n",
    "    def recol_ppi(df):\r\n",
    "        a = df.dev_ppi.fillna(0).values\r\n",
    "        b = df.dev_ppi_pred.fillna(0).values\r\n",
    "        c = []\r\n",
    "        # print(a,b)\r\n",
    "        for i in range(len(a)):\r\n",
    "            c.append(max(a[i], b[i]))\r\n",
    "        c = np.array(c)\r\n",
    "        df[\"final_ppi\"] = c\r\n",
    "        df[\"final_ppi\"][df[\"final_ppi\"] == 0] = None\r\n",
    "        return df\r\n",
    "\r\n",
    "    df1 = recol_ppi(df1)\r\n",
    "    df2 = recol_ppi(df2)\r\n",
    "    gc.collect()\r\n",
    "    return df1, df2\r\n",
    "\r\n",
    "def process_osv(df1, df2):\r\n",
    "    def process_osv1(x):\r\n",
    "        x = str(x)\r\n",
    "        if not x:\r\n",
    "            return -1\r\n",
    "        elif x.startswith(\"Android\"):\r\n",
    "            x = str(re.findall(\"\\d{1}\\.*\\d*\\.*\\d*\", x)[0])\r\n",
    "            return x\r\n",
    "        elif x.isdigit():\r\n",
    "            return x\r\n",
    "        else:\r\n",
    "            try:\r\n",
    "                x = str(re.findall(\"\\d{1}\\.\\d\\.*\\d*\", x)[0])\r\n",
    "                return x\r\n",
    "            except:\r\n",
    "                return 0\r\n",
    "\r\n",
    "    df1.osv = df1.osv.apply(process_osv1)\r\n",
    "    df2.osv = df2.osv.apply(process_osv1)\r\n",
    "    set3 = set(df1[\"osv\"].value_counts().head(70).index)\r\n",
    "\r\n",
    "    def process_osv2(x):\r\n",
    "        if x in set3:\r\n",
    "            return x\r\n",
    "        else:\r\n",
    "            return 0\r\n",
    "\r\n",
    "    df1[\"osv\"] = df1[\"osv\"].apply(process_osv2)\r\n",
    "    df2[\"osv\"] = df2[\"osv\"].apply(process_osv2)\r\n",
    "\r\n",
    "    le8 = preprocessing.LabelEncoder()\r\n",
    "    df1.osv = le8.fit_transform(df1.osv.astype(\"str\"))\r\n",
    "    df1[\"osv\"] = df1[\"osv\"].astype(\"object\")\r\n",
    "\r\n",
    "    df2.osv = le8.transform(df2.osv.astype(\"str\"))\r\n",
    "    df2[\"osv\"] = df2[\"osv\"].astype(\"object\")\r\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 特征工程处理\r\n",
    "df = data_preprocessing(train,test)\r\n",
    "df1 = df[0]\r\n",
    "df2 = df[1]\r\n",
    "\r\n",
    "for col in [\"location\", \"os\", \"ntt\", \"cus_type\"]:\r\n",
    "    df1, df2 = process_category(df1, df2, col)\r\n",
    "for col, dict1 in zip([\"carrier\"], [{0.0: 0, 46000.0: 1, 46001.0: 2, 46003.0: 3, -1.0: -1}]):\r\n",
    "    df1, df2 = dict_category(df1, df2, col, dict1)\r\n",
    "for col in [\"apptype\", \"media_id\", \"version\", \"lan\", \"package\", \"fea1_hash\", \"fea_hash\"]:\r\n",
    "    df1, df2 = special_category(df1, df2, col)\r\n",
    "\r\n",
    "df1, df2 = process_osv(df1, df2)\r\n",
    "df1, df2 = rf_cast(df1, df2)\r\n",
    "df1, df2 = feature(df1, df2)\r\n",
    "\r\n",
    "df1.to_pickle(\"/home/aistudio/processed_data/train.jlz\")\r\n",
    "df2.to_pickle(\"/home/aistudio/processed_data/test.jlz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3 模型训练与预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义catboost训练和预测函数\r\n",
    "def catboost_train_predict(train_path,test_path):\r\n",
    "    feature_train = pd.read_pickle(train_path)  # 训练\r\n",
    "    feature_test = pd.read_pickle(test_path)    # 测试\r\n",
    "    # 特征类别转换\r\n",
    "    for col in [\"dev_height\", \"dev_width\", \"hw_ratio\", \"hw_matrix\", \"inch\", \"lan\"]:\r\n",
    "        if col in feature_train.columns:\r\n",
    "            feature_train[col] = feature_train[col].astype(\"float64\")\r\n",
    "            feature_test[col] = feature_test[col].astype(\"float64\")\r\n",
    "    \r\n",
    "    # 所使用的特征\r\n",
    "    cate_feature = ['apptype', 'carrier', 'media_id', 'os', 'osv', 'package', 'version', 'location', 'cus_type',\r\n",
    "                    \"fea1_hash\", \"fea_hash\", \"ntt\", \"os\", 'fea1_hash_ntt_combine', 'fea_hash_carrier_combine',\r\n",
    "                    'cus_type_osv_combine', 'fea1_hash_apptype_combine', 'fea_hash_media_id_combine',\r\n",
    "                    'cus_type_version_combine', 'apptype_ntt_combine', 'media_id_carrier_combine',\r\n",
    "                    'version_osv_combine', 'package_lan_combine', 'lan']\r\n",
    "\r\n",
    "    y_col = 'label'\r\n",
    "    x_col = ['apptype', 'carrier', 'dev_height',\r\n",
    "             'dev_width', 'lan', 'media_id', 'ntt', 'osv', 'package',\r\n",
    "             'timestamp', 'version', 'fea_hash', 'location', 'fea1_hash', 'cus_type',\r\n",
    "             'hour', 'minute',\r\n",
    "             '160_height',\r\n",
    "             'hw_ratio', 'hw_matrix', 'inch']\r\n",
    "\r\n",
    "    cate_feature = [x for x in cate_feature if x in x_col]\r\n",
    "    for item in cate_feature:\r\n",
    "        if item in ['fea1_hash_ntt_combine', 'fea_hash_carrier_combine', 'cus_type_osv_combine',\r\n",
    "                    'fea1_hash_apptype_combine', 'fea_hash_media_id_combine', 'cus_type_version_combine',\r\n",
    "                    'apptype_ntt_combine', 'media_id_carrier_combine', 'version_osv_combine', 'package_lan_combine']:\r\n",
    "            set4 = set(feature_train[item].value_counts().head(300).index)\r\n",
    "\r\n",
    "            def process_fea_hash(x):\r\n",
    "                if x in set4:\r\n",
    "                    return x\r\n",
    "                else:\r\n",
    "                    return -1\r\n",
    "\r\n",
    "            feature_train[item] = feature_train[item].apply(process_fea_hash).astype(\"str\")\r\n",
    "            feature_test[item] = feature_test[item].apply(process_fea_hash).astype(\"str\")\r\n",
    "        le = preprocessing.LabelEncoder()\r\n",
    "        feature_train[item] = le.fit_transform(feature_train[item])\r\n",
    "        feature_test[item] = le.transform(feature_test[item])\r\n",
    "\r\n",
    "    df_prediction = feature_test[x_col]\r\n",
    "    df_prediction['label'] = 0\r\n",
    "\r\n",
    "    # 树模型参数设置：通过控制变量的方式进行动态调整\r\n",
    "    model = CatBoostClassifier(\r\n",
    "        loss_function=\"Logloss\",    # 分类任务常用损失函数\r\n",
    "        eval_metric=\"Accuracy\",     # 表示用于过度拟合检测和最佳模型选择的度量标准；\r\n",
    "        learning_rate=0.08,         # 表示学习率\r\n",
    "        iterations=10000,\r\n",
    "        random_seed=42,           # 设置随机种子进行固定\r\n",
    "        od_type=\"Iter\",\r\n",
    "        metric_period=20,           # 与交叉验证folds数匹配\r\n",
    "        max_depth = 8,              # 表示树模型最大深度\r\n",
    "        early_stopping_rounds=500,  # 早停步数\r\n",
    "        use_best_model=True,\r\n",
    "        # task_type=\"GPU\",          # 数据量较小，GPU加速效果不明显\r\n",
    "        bagging_temperature=0.9,\r\n",
    "        leaf_estimation_method=\"Newton\",\r\n",
    "    )\r\n",
    "\r\n",
    "    li_f = []\r\n",
    "    df_importance_list = []\r\n",
    "    n = 20  # 设置20折交叉验证\r\n",
    "    kfold = KFold(n_splits=n, shuffle=True, random_state=42)\r\n",
    "    # weight = [0.1, 0.11, 0.1, 0.11, 0.11, 0.11, 0.05, 0.11, 0.1, 0.1]\r\n",
    "    # assert sum(weight) == 1 and len(weight) == n\r\n",
    "    for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(feature_train[x_col], feature_train[y_col])):\r\n",
    "        X_train = feature_train.iloc[trn_idx][x_col]\r\n",
    "        Y_train = feature_train.iloc[trn_idx][y_col]\r\n",
    "\r\n",
    "        X_val = feature_train.iloc[val_idx][x_col]\r\n",
    "        Y_val = feature_train.iloc[val_idx][y_col]\r\n",
    "\r\n",
    "        print('\\nFold_{} Training ================================\\n'.format(fold_id + 1))\r\n",
    "        cat_model = model.fit(\r\n",
    "            X_train,\r\n",
    "            Y_train,\r\n",
    "            cat_features=cate_feature,  # 特征\r\n",
    "            # # eval_names=['train', 'valid'],\r\n",
    "            eval_set=(X_val, Y_val),\r\n",
    "            verbose=100,\r\n",
    "            # plot=True\r\n",
    "            # eval_metric=[\"auc\",\"binary_logloss\",\"binary_error\"],\r\n",
    "            # early_stopping_rounds=400\r\n",
    "        )\r\n",
    "\r\n",
    "        pred_val = cat_model.predict_proba(X_val, thread_count=-1)[:, 1]\r\n",
    "        df_oof = feature_train.iloc[val_idx].copy()\r\n",
    "        df_oof['pred'] = pred_val\r\n",
    "        li_f.append(df_oof)\r\n",
    "\r\n",
    "        pred_test = cat_model.predict_proba(feature_test[x_col], thread_count=-1)[:, 1]\r\n",
    "        df_prediction['label'] += pred_test / n\r\n",
    "\r\n",
    "        df_importance = pd.DataFrame({\r\n",
    "            'column': x_col,\r\n",
    "            'importance': cat_model.feature_importances_,\r\n",
    "        })\r\n",
    "        df_importance_list.append(df_importance)\r\n",
    "    return df_prediction, li_f, feature_train, feature_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义预测结果保存函数\r\n",
    "def save(file_path, pred, df1, df2, threshold=0.5):\r\n",
    "    a = pd.DataFrame(pred.index)\r\n",
    "    a['label'] = pred[\"label\"].values\r\n",
    "\r\n",
    "    # 由于输出结果为0或1，故需要对分数结果进行后处理操作：大于threshold的为1，小于或等于threshold则为0。 threshold为设定的阈值\r\n",
    "    a.label = a.label.apply(lambda x: 1 if x > threshold else 0)\r\n",
    "    user_label = pd.DataFrame()\r\n",
    "\r\n",
    "    user_label[\"uid\"] = df1.android_id.values\r\n",
    "    user_label[\"ntt\"] = df1.ntt.values\r\n",
    "    temp = pd.DataFrame(df1.groupby([\"android_id\", \"ntt\"]).label.mean())\r\n",
    "    temp = temp.reset_index()\r\n",
    "    temp.rename(columns={\"android_id\": \"uid\", \"label\": \"label_prior\"}, inplace=True)\r\n",
    "    user_label = pd.merge(user_label, temp, on=[\"uid\", \"ntt\"], how=\"left\")\r\n",
    "    user_label.drop_duplicates(inplace=True)\r\n",
    "    a[\"uid\"] = df2.android_id.values\r\n",
    "    a[\"ntt\"] = df2.ntt.values\r\n",
    "    a = pd.merge(a, user_label, how=\"left\", on=[\"uid\", \"ntt\"])\r\n",
    "\r\n",
    "    def post(label, prior):\r\n",
    "        n = len(label)\r\n",
    "        count = 0\r\n",
    "        for i in range(n):\r\n",
    "            if 0 <= prior[i] <= 0.1 and label[i] == 1:\r\n",
    "                label[i] = 0\r\n",
    "                count += 1\r\n",
    "            elif 0.9 <= prior[i] <= 1 and label[i] == 0:\r\n",
    "                label[i] = 1\r\n",
    "                count += 1\r\n",
    "            else:\r\n",
    "                pass\r\n",
    "        print(count)\r\n",
    "        return label.values\r\n",
    "\r\n",
    "    a.label = post(a.label, a.label_prior)\r\n",
    "    a = a[[\"sid\", \"label\"]]\r\n",
    "    a.to_csv(file_path, index=False)\r\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 进行树模型的训练和预测\r\n",
    "df_prediction, li_f, feature_train, feature_test  = catboost_train_predict(\"/home/aistudio/processed_data/train.jlz\",\"/home/aistudio/processed_data/test.jlz\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存预测结果文件\r\n",
    "filename = './submission.csv'  # 设置保存结果文件名\r\n",
    "# 可以通过修改threshold的值来修改阈值\r\n",
    "save(filename, df_prediction, feature_train, feature_test,threshold=0.5)\r\n",
    "print(\"success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "前往比赛页面提交主页面里生成的结果文件：**submission.csv**文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看结果文件格式是否符合要求：sid,label\r\n",
    "result = pd.read_csv('./submission.csv')\r\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四.提升方向\n",
    "\n",
    "**1.特征工程:**\n",
    "\n",
    "数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。特征工程作为数据挖掘比赛的一大重要提升点，需要结合数据分析多做尝试。\n",
    "\n",
    "**2.模型调参优化:**\n",
    "\n",
    "调整树模型的参数设置以及结果后处理的阈值等以提升模型效果。可以考虑使用[Optuna](https://mp.weixin.qq.com/s/Gzl288KbqL785FwCZJWIew)或GridSearchCV等自动调参工具包帮助寻找模型最优参数。\n",
    "\n",
    "**3.多模型结果融合：**\n",
    "\n",
    "可以尝试使用不同的树模型或基于Paddle使用深度模型（[DeepFm](https://aistudio.baidu.com/aistudio/projectdetail/1231397)等），再对多模型的结果根据分数排名进行加权[融合](https://aistudio.baidu.com/aistudio/projectdetail/2315563)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
